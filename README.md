# ğŸ˜ƒ Mood Detector Playground

Real-time **facial emotion detection** powered by **TensorFlow, OpenCV, and Flask**. Detect emotions from your webcam directly in the browser or run a local version with **speech feedback**. Includes two versions: a **live TFLite deployment** and a **local .h5-based demo**.

---

## ğŸš€ Features

* Detects **7 emotions**: Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise
* **Real-time webcam detection**
* **Two modes**:

  * ğŸŸ¢ `app_r.py` â†’ Lightweight TFLite model for deployment
  * ğŸ–¥ï¸ `app.py` â†’ Local version with speech feedback (using `.h5`)
* **Mediapipe Mesh Demo (`main.py`)** â†’ Rule-based emotion estimation using mathematical facial landmark calculations (no deep learning)
* **Web Interface** with modern UI (`home.html`, `detect.html`)
* **Secure & Private** â†’ All inference happens locally in-browser

---

## ğŸ—‚ï¸ Project Structure

```
MOOD-DETECTOR-PLAYGROUND/
â”‚
â”œâ”€â”€ Models/
â”‚   â”œâ”€â”€ emotion_model.h5          # Keras model (local version)
â”‚   â””â”€â”€ emotion_model.tflite      # TFLite model (deployment)
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ hero-emotion-ai.jpg
â”‚   â”œâ”€â”€ scripts.js
â”‚   â””â”€â”€ styles.css
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ home.html
â”‚   â””â”€â”€ detect.html
â”‚
â”œâ”€â”€ app.py                        # Local version (.h5, with TTS)
â”œâ”€â”€ app_r.py                      # Deployment version (TFLite)
â”œâ”€â”€ main.py                       # Mediapipe math-based demo
â”œâ”€â”€ model_train.py                 # Model training script
â”œâ”€â”€ model_train.ipynb              # Training notebook with results
â”œâ”€â”€ requirements.txt
â””â”€â”€ render.yaml (optional)
```

---

## ğŸ“Š Model Training

The CNN model was trained using the **Face Expression Recognition Dataset** (7 emotion classes).

### ğŸ”¹ Dataset

* Train/Test split: 80/20
* Image preprocessing:

  * Grayscale conversion
  * Resized to **48Ã—48**
  * Normalized pixel values (0â€“1)

### ğŸ”¹ Model Architecture

* **Conv2D (64) â†’ BatchNorm â†’ MaxPool â†’ Dropout (0.25)**
* **Conv2D (128) â†’ BatchNorm â†’ MaxPool â†’ Dropout (0.25)**
* **Conv2D (256) â†’ BatchNorm â†’ MaxPool â†’ Dropout (0.25)**
* **Flatten â†’ Dense(512, ReLU) â†’ Dropout (0.5)**
* **Dense(7, Softmax)**

### ğŸ”¹ Training

* Optimizer: **Adam (lr=0.001)**
* Loss: **Categorical Crossentropy (with label smoothing)**
* Data Augmentation: rotation (10Â°), zoom (10%), horizontal flips, shifts.
* Callbacks:

  * `EarlyStopping` (patience=5)
  * `ReduceLROnPlateau`
  * `ModelCheckpoint` (best weights only)

### ğŸ”¹ Results

* **Validation Accuracy**: \~56%
* **Confusion Matrix**: Strong on *Happy* & *Surprise*, weaker on *Disgust* & *Fear*
* Predictions plotted against ground truth in notebook

### ğŸ”¹ Outputs

* Final model saved as **`emotions_model3.h5`**
* Converted later into **`emotion_model.tflite`** for deployment

---

## â–¶ï¸ Running the App

### 1. Local version (`app.py`)

Runs the `.h5` model with **text-to-speech feedback**.

```bash
pip install -r requirements.txt
python app.py
```

Open in browser: `http://127.0.0.1:5000/`

### 2. Deployment version (`app_r.py`)

Runs the lightweight **TFLite model** (for Render/production).

```bash
pip install -r requirements.txt
python app_r.py
```

---

## ğŸ§ª Mediapipe Demo (`main.py`)

This is an **alternative experiment** that doesnâ€™t rely on CNN training. Instead, it uses:

* **FaceMesh landmarks** from Mediapipe
* **Mathematical ratios** of lip, eye, and iris distances
* Rule-based classification of moods (Happy, Sad, Angry, Surprised, Neutral, Fearful, Disgusted)
* Displays **webcam + avatar mesh side by side**

This version demonstrates how emotions can be inferred using **geometric features** instead of deep learning.

---

## ğŸ“¦ Requirements

```
Flask
tensorflow
numpy
opencv-python-headless
Pillow
gunicorn
```
